{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changing working directory to repository path \n",
    "in order to make simpler references to files/folder.\n",
    "\n",
    "Also, adding src folder in the repository to import\n",
    "any code that has been moved to py files for reusability\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "REPOSITORY_PATH = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/mlgpu2/code/Users/santiago.a.diez/evaluating-student-writing-kaggle-challenge'\n",
    "os.chdir(REPOSITORY_PATH)\n",
    "import sys  \n",
    "sys.path.insert(0, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ast import literal_eval\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "#from transformers import *\n",
    "\n",
    "from eswkg.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_essay(essay_id, train_folder = Config.get_all_file_paths()[\"train_folder\"]):\n",
    "    with open(train_folder + f\"/{essay_id}.txt\") as f:\n",
    "        essay = f.read()\n",
    "    return essay\n",
    "\n",
    "\n",
    "def read_essays(train_txt):\n",
    "    train_txt_file_id, train_txt_file_text = [],[]\n",
    "    for train_txt_file in train_txt:\n",
    "        essay_id = os.path.basename(train_txt_file).rsplit(\".\",1)[0]\n",
    "\n",
    "        train_txt_file_id.append(essay_id)\n",
    "        train_txt_file_text.append(read_essay(essay_id))\n",
    "    return pd.DataFrame({\"id\":train_txt_file_id, \"text\":train_txt_file_text})\n",
    "\n",
    "\n",
    "def get_essay_entities(essay_text, essay_metadata):\n",
    "    essay_entities = [\"O\"]*len(essay_text.split())\n",
    "    for discourse_type, predictionstring in zip(essay_metadata[\"discourse_type\"],essay_metadata[\"predictionstring\"]):\n",
    "        predictionstring_digits = list(map(int, predictionstring.split()))\n",
    "\n",
    "        essay_entities[predictionstring_digits[0]] = f\"B-{discourse_type}\"\n",
    "        for predictionstring_digits_index in predictionstring_digits[1:]:\n",
    "           essay_entities[predictionstring_digits_index] = f\"I-{discourse_type}\"\n",
    "    \n",
    "    return essay_entities\n",
    "\n",
    "\n",
    "def tag_essays(essays, essays_metadata):\n",
    "    tagged_essays = pd.DataFrame()\n",
    "    tagged_essays_list = []\n",
    "    for _, essay in essays.iterrows():\n",
    "        essay_id = essay[\"id\"]\n",
    "        essay_text = essay[\"text\"]\n",
    "        essay_metadata = essays_metadata.query(\"id == @essay_id\")\n",
    "        essay_entities = get_essay_entities(essay_text, essay_metadata)\n",
    "\n",
    "        tagged_essays_list.append( \n",
    "            {\n",
    "                \"id\": essay_id,\n",
    "                \"text\": essay_text,\n",
    "                \"entities\": essay_entities\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame.from_dict(tagged_essays_list)\n",
    "\n",
    "\n",
    "def generate_file(generation_func, file_path, generate_file=False, *args):\n",
    "    try:\n",
    "        if generate_file:\n",
    "            generation_func(*args).to_csv(file_path, index=False)\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as err:\n",
    "        print(f\"{err}, {type(err)}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Unexpected {err}, {type(err)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving file paths for different folders and files in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = Config.get_all_file_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_metadata = pd.read_csv(file_paths[\"train\"])\n",
    "essays_metadata[['discourse_id', 'discourse_start', 'discourse_end']] = essays_metadata[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n",
    "\n",
    "sample_submission = pd.read_csv(file_paths[\"sample_submission\"])\n",
    "\n",
    "#The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\n",
    "train_txt = glob(file_paths[\"train_folder\"] + \"/*.txt\") \n",
    "test_txt = glob(file_paths[\"test_folder\"] + \"/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...\n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...\n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...\n",
       "3  001552828BD0  Would you be able to give your car up? Having ...\n",
       "4  0016926B079C  I think that students would benefit from learn..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_essays_file = False\n",
    "essays_file_path = file_paths[\"intermediate\"]+\"/train_text.csv\"\n",
    "\n",
    "essays = generate_file(read_essays, essays_file_path, create_essays_file, train_txt)\n",
    "\n",
    "print(essays.shape)\n",
    "essays.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "      <td>[O, O, B-Position, I-Position, I-Position, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...   \n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...   \n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...   \n",
       "3  001552828BD0  Would you be able to give your car up? Having ...   \n",
       "4  0016926B079C  I think that students would benefit from learn...   \n",
       "\n",
       "                                            entities  \n",
       "0  [B-Position, I-Position, I-Position, I-Positio...  \n",
       "1  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "2  [O, O, B-Position, I-Position, I-Position, I-P...  \n",
       "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "4  [B-Position, I-Position, I-Position, I-Positio...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_essay_entities_file = False\n",
    "essay_entities_file_path = file_paths[\"model_input\"]+\"/essays_NER.csv\"\n",
    "\n",
    "essays_entities = generate_file(tag_essays, essay_entities_file_path, create_essay_entities_file, essays, essays_metadata)\n",
    "essays_entities.entities = essays_entities.entities.apply(lambda x: literal_eval(x) )\n",
    "\n",
    "print(essays_entities.shape)\n",
    "essays_entities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-Lead': 1,\n",
       " 'I-Lead': 2,\n",
       " 'B-Position': 3,\n",
       " 'I-Position': 4,\n",
       " 'B-Evidence': 5,\n",
       " 'I-Evidence': 6,\n",
       " 'B-Claim': 7,\n",
       " 'I-Claim': 8,\n",
       " 'B-Concluding Statement': 9,\n",
       " 'I-Concluding Statement': 10,\n",
       " 'B-Counterclaim': 11,\n",
       " 'I-Counterclaim': 12,\n",
       " 'B-Rebuttal': 13,\n",
       " 'I-Rebuttal': 14}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = []\n",
    "label_list.append('O')\n",
    "\n",
    "for discourse_type in essays_metadata.discourse_type.unique():\n",
    "    label_list.append(f'B-{discourse_type}')\n",
    "    label_list.append(f'I-{discourse_type}')\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(label_list)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(label_list)}\n",
    "\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mapping(unsplit):\n",
    "    # Return an array that maps character index to index of word in list of split() words\n",
    "    # Code copied from https://www.kaggle.com/chasembowers/pytorch-bigbird-whitespace-cv-0-6284/notebook\n",
    "    splt = unsplit.split()\n",
    "    offset_to_wordidx = np.full(len(unsplit),-1)\n",
    "    txt_ptr = 0\n",
    "    for split_index, full_word in enumerate(splt):\n",
    "        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n",
    "            txt_ptr += 1\n",
    "        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n",
    "        txt_ptr += len(full_word)\n",
    "    return offset_to_wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, max_len):\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.sentences[index]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        word_ids = encoding.word_ids()\n",
    "        split_word_ids = np.full(len(word_ids),-1)\n",
    "        offset_to_wordidx = split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "        \n",
    "        mask = encoding['attention_mask']\n",
    "        label = self.labels[index]\n",
    "        label.extend([4]*200)\n",
    "        label=label[:200]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(word_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'tags': torch.tensor(label, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "sentences = essays_entities.text\n",
    "labels = essays_entities.entities\n",
    "max_len = 1024\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
    "training_set = dataset(\n",
    "    tokenizer=tokenizer, \n",
    "    sentences=sentences, \n",
    "    labels=labels, \n",
    "    max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "      <td>[O, O, B-Position, I-Position, I-Position, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...   \n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...   \n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...   \n",
       "3  001552828BD0  Would you be able to give your car up? Having ...   \n",
       "4  0016926B079C  I think that students would benefit from learn...   \n",
       "\n",
       "                                            entities  \n",
       "0  [B-Position, I-Position, I-Position, I-Positio...  \n",
       "1  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "2  [O, O, B-Position, I-Position, I-Position, I-P...  \n",
       "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "4  [B-Position, I-Position, I-Position, I-Positio...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa. It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.\n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth. This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.\n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world. These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention. NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.\n",
      "['Some', 'Ġpeople', 'Ġbel', 'ive', 'Ġthat', 'Ġthe', 'Ġso', 'Ġcalled', 'Ġ\"', 'face', '\"', 'Ġon', 'Ġmars', 'Ġwas', 'Ġcreated', 'Ġby', 'Ġlife', 'Ġon', 'Ġmars', '.', 'ĠThis', 'Ġis', 'Ġnot', 'Ġthe', 'Ġcase', '.', 'ĠThe', 'Ġface', 'Ġon', 'ĠMars', 'Ġis', 'Ġa', 'Ġnaturally', 'Ġocc', 'uring', 'Ġland', 'Ġform', 'Ġcalled', 'Ġa', 'Ġmes', 'a', '.', 'ĠIt', 'Ġwas', 'Ġnot', 'Ġcreated', 'Ġby', 'Ġaliens', ',', 'Ġand', 'Ġthere', 'Ġis', 'Ġno', 'Ġcons', 'ir', 'acy', 'Ġto', 'Ġhide', 'Ġalien', 'Ġlife', 'forms', 'Ġon', 'Ġmars', '.', 'ĠThere', 'Ġis', 'Ġno', 'Ġevidence', 'Ġthat', 'ĠNASA', 'Ġhas', 'Ġfound', 'Ġthat', 'Ġeven', 'Ġsuggests', 'Ġthat', 'Ġthis', 'Ġface', 'Ġwas', 'Ġcreated', 'Ġby', 'Ġaliens', '.', 'Ċ', 'Ċ', 'A', 'Ġmes', 'a', 'Ġis', 'Ġa', 'Ġnaturally', 'Ġocc', 'uring', 'Ġrock', 'Ġformation', ',', 'Ġthat', 'Ġis', 'Ġfound', 'Ġon', 'ĠMars', 'Ġand', 'ĠEarth', '.', 'ĠThis', 'Ġ\"', 'face', '\"', 'Ġon', 'Ġmars', 'Ġonly', 'Ġlooks', 'Ġlike', 'Ġa', 'Ġface', 'Ġbecause', 'Ġhumans', 'Ġtend', 'Ġto', 'Ġsee', 'Ġfaces', 'Ġwherever', 'Ġwe', 'Ġlook', ',', 'Ġhumans', 'Ġare', 'Ġobviously', 'Ġextremely', 'Ġsocial', ',', 'Ġwhich', 'Ġis', 'Ġwhy', 'Ġour', 'Ġbrain', 'Ġis', 'Ġdesigned', 'Ġto', 'Ġrecognize', 'Ġfaces', '.', 'Ċ', 'Ċ', 'Many', 'Ġconspiracy', 'Ġtheorists', 'Ġbelieve', 'Ġthat', 'ĠNASA', 'Ġis', 'Ġhiding', 'Ġlife', 'Ġon', 'ĠMars', 'Ġfrom', 'Ġthe', 'Ġrest', 'Ġof', 'Ġthe', 'Ġworld', '.', 'ĠThese', 'Ġpeople', 'Ġwould', 'Ġbe', 'Ġvery', 'Ġwrong', '.', 'ĠIf', 'ĠNASA', 'Ġfound', 'Ġlife', 'Ġon', 'ĠMars', ',', 'Ġthen', 'Ġthey', 'Ġwould', 'Ġget', 'Ġmillions', 'Ġof', 'Ġpeople', \"'s\", 'Ġattention', '.', 'ĠNASA', \"'s\", 'Ġbudget', 'Ġwould', 'Ġincrease', 'Ġdrastic', 'ly', ',', 'Ġwhich', 'Ġmeans', 'Ġthat', 'Ġtheir', 'Ġworkers', 'Ġwould', 'Ġget', 'Ġpaid', 'Ġmore', '.', 'ĠThere', 'Ġis', 'Ġno', 'Ġgood', 'Ġreason', 'Ġthat', 'ĠNASA', 'Ġwould', 'Ġhide', 'Ġlife', 'Ġon', 'ĠMars', 'Ġfrom', 'Ġthe', 'Ġrest', 'Ġof', 'Ġthe', 'Ġworld', '.', 'Ċ', 'Ċ', 'So', ',', 'ĠNASA', 'Ġis', 'Ġnot', 'Ġhiding', 'Ġlife', 'Ġon', 'ĠMars', 'Ġfrom', 'Ġus', ',', 'Ġand', 'Ġthey', 'Ġare', 'Ġnot', 'Ġtrying', 'Ġto', 'Ġtrick', 'Ġus', 'Ġinto', 'Ġthinking', 'Ġthat', 'Ġthe', 'Ġ\"', 'face', '\"', 'Ġon', 'Ġmars', 'Ġis', 'Ġjust', 'Ġa', 'Ġmes', 'a', ',', 'Ġbecause', 'Ġit', 'Ġactually', 'Ġis', '.', 'ĠNASA', 'Ġhiding', 'Ġlife', 'Ġwould', 'Ġbe', 'Ġill', 'ogical', ',', 'Ġbecause', 'Ġif', 'Ġthey', 'Ġfound', 'Ġlife', 'Ġon', 'ĠMars', ',', 'Ġthey', 'Ġwould', 'Ġmake', 'Ġa', 'Ġlot', 'Ġof', 'Ġmoney', ',', 'Ġand', 'Ġwe', 'Ġall', 'Ġknow', 'Ġthat', 'Ġthe', 'Ġpeople', 'Ġat', 'ĠNASA', 'Ġaren', \"'t\", 'Ġill', 'ogical', 'Ġpeople', '.']\n",
      "304\n",
      "[None, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 32, 33, 34, 35, 36, 37, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 50, 50, 51, 52, 53, 54, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 80, 81, 82, 83, 84, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 289, 290, 291, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[(0, 0), (0, 4), (5, 11), (12, 15), (15, 18), (19, 23), (24, 27), (28, 30), (31, 37), (38, 39), (39, 43), (43, 44), (45, 47), (48, 52), (53, 56), (57, 64), (65, 67), (68, 72), (73, 75), (76, 80), (80, 81), (82, 86), (87, 89), (90, 93), (94, 97), (98, 102), (102, 103), (104, 107), (108, 112), (113, 115), (116, 120), (121, 123), (124, 125), (126, 135), (136, 139), (139, 144), (145, 149), (150, 154), (155, 161), (162, 163), (164, 167), (167, 168), (168, 169), (170, 172), (173, 176), (177, 180), (181, 188), (189, 191), (192, 198), (198, 199), (200, 203), (204, 209), (210, 212), (213, 215), (216, 220), (220, 222), (222, 225), (226, 228), (229, 233), (234, 239), (240, 244), (244, 249), (250, 252), (253, 257), (257, 258), (259, 264), (265, 267), (268, 270), (271, 279), (280, 284), (285, 289), (290, 293), (294, 299), (300, 304), (305, 309), (310, 318), (319, 323), (324, 328), (329, 333), (334, 337), (338, 345), (346, 348), (349, 355), (355, 356), (356, 357), (357, 358), (358, 359), (360, 363), (363, 364), (365, 367), (368, 369), (370, 379), (380, 383), (383, 388), (389, 393), (394, 403), (403, 404), (405, 409), (410, 412), (413, 418), (419, 421), (422, 426), (427, 430), (431, 436), (436, 437), (438, 442), (443, 444), (444, 448), (448, 449), (450, 452), (453, 457), (458, 462), (463, 468), (469, 473), (474, 475), (476, 480), (481, 488), (489, 495), (496, 500), (501, 503), (504, 507), (508, 513), (514, 522), (523, 525), (526, 530), (530, 531), (532, 538), (539, 542), (543, 552), (553, 562), (563, 569), (569, 570), (571, 576), (577, 579), (580, 583), (584, 587), (588, 593), (594, 596), (597, 605), (606, 608), (609, 618), (619, 624), (624, 625), (625, 626), (626, 627), (627, 631), (632, 642), (643, 652), (653, 660), (661, 665), (666, 670), (671, 673), (674, 680), (681, 685), (686, 688), (689, 693), (694, 698), (699, 702), (703, 707), (708, 710), (711, 714), (715, 720), (720, 721), (722, 727), (728, 734), (735, 740), (741, 743), (744, 748), (749, 754), (754, 755), (756, 758), (759, 763), (764, 769), (770, 774), (775, 777), (778, 782), (782, 783), (784, 788), (789, 793), (794, 799), (800, 803), (804, 812), (813, 815), (816, 822), (822, 824), (825, 834), (834, 835), (836, 840), (840, 842), (843, 849), (850, 855), (856, 864), (865, 872), (872, 874), (874, 875), (876, 881), (882, 887), (888, 892), (893, 898), (899, 906), (907, 912), (913, 916), (917, 921), (922, 926), (926, 927), (928, 933), (934, 936), (937, 939), (940, 944), (945, 951), (952, 956), (957, 961), (962, 967), (968, 972), (973, 977), (978, 980), (981, 985), (986, 990), (991, 994), (995, 999), (1000, 1002), (1003, 1006), (1007, 1012), (1012, 1013), (1013, 1014), (1014, 1015), (1015, 1017), (1017, 1018), (1019, 1023), (1024, 1026), (1027, 1030), (1031, 1037), (1038, 1042), (1043, 1045), (1046, 1050), (1051, 1055), (1056, 1058), (1058, 1059), (1060, 1063), (1064, 1068), (1069, 1072), (1073, 1076), (1077, 1083), (1084, 1086), (1087, 1092), (1093, 1095), (1096, 1100), (1101, 1109), (1110, 1114), (1115, 1118), (1119, 1120), (1120, 1124), (1124, 1125), (1126, 1128), (1129, 1133), (1134, 1136), (1137, 1141), (1142, 1143), (1144, 1147), (1147, 1148), (1148, 1149), (1150, 1157), (1158, 1160), (1161, 1169), (1170, 1172), (1172, 1173), (1174, 1178), (1179, 1185), (1186, 1190), (1191, 1196), (1197, 1199), (1200, 1203), (1203, 1209), (1209, 1210), (1211, 1218), (1219, 1221), (1222, 1226), (1227, 1232), (1233, 1237), (1238, 1240), (1241, 1245), (1245, 1246), (1247, 1251), (1252, 1257), (1258, 1262), (1263, 1264), (1265, 1268), (1269, 1271), (1272, 1277), (1277, 1278), (1279, 1282), (1283, 1285), (1286, 1289), (1290, 1294), (1295, 1299), (1300, 1303), (1304, 1310), (1311, 1313), (1314, 1318), (1319, 1323), (1323, 1325), (1326, 1329), (1329, 1335), (1336, 1342), (1342, 1343), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# GET TEXT AND WORD LABELS \n",
    "index = 0 \n",
    "text = essays_entities.text[index]        \n",
    "get_wids = False\n",
    "word_labels = essays_entities.entities[index] if not get_wids else None\n",
    "\n",
    "# TOKENIZE TEXT\n",
    "encoding = tokenizer(text,\n",
    "                        return_offsets_mapping=True, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        max_length=max_len)\n",
    "\n",
    "word_ids = encoding.word_ids()  \n",
    "split_word_ids = np.full(len(word_ids),-1)\n",
    "offset_to_wordidx = split_mapping(text)\n",
    "offsets = encoding['offset_mapping']\n",
    "\n",
    "print(len(text.split()))\n",
    "print(text)\n",
    "\n",
    "print(tokenizer.tokenize(text))\n",
    "print(len(tokenizer.tokenize(text)))\n",
    "\n",
    "print(word_ids)\n",
    "print(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "label_ids = []\n",
    "# Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "    if word_idx is None:\n",
    "        if not get_wids: label_ids.append(-100)\n",
    "    else:\n",
    "        if offsets[token_idx] != (0,0):\n",
    "            #Choose the split word that shares the most characters with the token if any\n",
    "            split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "            split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "            \n",
    "            if split_index != -1: \n",
    "                if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n",
    "                split_word_ids[token_idx] = split_index\n",
    "            else:\n",
    "                # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n",
    "                    split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                    if not self.get_wids: label_ids.append(label_ids[-1])\n",
    "                else:\n",
    "                    if not self.get_wids: label_ids.append(-100)\n",
    "        else:\n",
    "            if not self.get_wids: label_ids.append(-100)\n",
    "\n",
    "encoding['labels'] = list(reversed(label_ids))\n",
    "\n",
    "# CONVERT TO TORCH TENSORS\n",
    "item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "if self.get_wids: \n",
    "    item['wids'] = torch.as_tensor(split_word_ids)\n",
    "\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids # for validation????\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        word_labels = self.data.entities[index] if not self.get_wids else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(text,\n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        word_ids = encoding.word_ids()  \n",
    "        split_word_ids = np.full(len(word_ids),-1)\n",
    "        offset_to_wordidx = split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "        \n",
    "        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "        label_ids = []\n",
    "        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "            \n",
    "            if word_idx is None:\n",
    "                if not self.get_wids: label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx] != (0,0):\n",
    "                    #Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "                    \n",
    "                    if split_index != -1: \n",
    "                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if not self.get_wids: label_ids.append(label_ids[-1])\n",
    "                        else:\n",
    "                            if not self.get_wids: label_ids.append(-100)\n",
    "                else:\n",
    "                    if not self.get_wids: label_ids.append(-100)\n",
    "        \n",
    "        encoding['labels'] = list(reversed(label_ids))\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids: \n",
    "            item['wids'] = torch.as_tensor(split_word_ids)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead\n",
      "Position\n",
      "Evidence\n",
      "Claim\n",
      "Concluding Statement\n",
      "Counterclaim\n",
      "Rebuttal\n"
     ]
    }
   ],
   "source": [
    "for type_ in essays_metadata.discourse_type.unique():\n",
    "    print(type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "azureml_py38_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
