{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changing working directory to repository path \n",
    "in order to make simpler references to files/folder.\n",
    "\n",
    "Also, adding src folder in the repository to import\n",
    "any code that has been moved to py files for reusability\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "REPOSITORY_PATH = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/mlgpu2/code/Users/santiago.a.diez/evaluating-student-writing-kaggle-challenge'\n",
    "os.chdir(REPOSITORY_PATH)\n",
    "import sys  \n",
    "sys.path.insert(0, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ast import literal_eval\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "#from transformers import *\n",
    "\n",
    "from eswkg.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_essay(essay_id, train_folder = Config.get_all_file_paths()[\"train_folder\"]):\n",
    "    with open(train_folder + f\"/{essay_id}.txt\") as f:\n",
    "        essay = f.read()\n",
    "    return essay\n",
    "\n",
    "\n",
    "def read_essays(train_txt):\n",
    "    train_txt_file_id, train_txt_file_text = [],[]\n",
    "    for train_txt_file in train_txt:\n",
    "        essay_id = os.path.basename(train_txt_file).rsplit(\".\",1)[0]\n",
    "\n",
    "        train_txt_file_id.append(essay_id)\n",
    "        train_txt_file_text.append(read_essay(essay_id))\n",
    "    return pd.DataFrame({\"id\":train_txt_file_id, \"text\":train_txt_file_text})\n",
    "\n",
    "\n",
    "def get_essay_entities(essay_text, essay_metadata):\n",
    "    essay_entities = [\"O\"]*len(essay_text.split())\n",
    "    for discourse_type, predictionstring in zip(essay_metadata[\"discourse_type\"],essay_metadata[\"predictionstring\"]):\n",
    "        predictionstring_digits = list(map(int, predictionstring.split()))\n",
    "\n",
    "        essay_entities[predictionstring_digits[0]] = f\"B-{discourse_type}\"\n",
    "        for predictionstring_digits_index in predictionstring_digits[1:]:\n",
    "           essay_entities[predictionstring_digits_index] = f\"I-{discourse_type}\"\n",
    "    \n",
    "    return essay_entities\n",
    "\n",
    "\n",
    "def tag_essays(essays, essays_metadata):\n",
    "    tagged_essays = pd.DataFrame()\n",
    "    tagged_essays_list = []\n",
    "    for _, essay in essays.iterrows():\n",
    "        essay_id = essay[\"id\"]\n",
    "        essay_text = essay[\"text\"]\n",
    "        essay_metadata = essays_metadata.query(\"id == @essay_id\")\n",
    "        essay_entities = get_essay_entities(essay_text, essay_metadata)\n",
    "\n",
    "        tagged_essays_list.append( \n",
    "            {\n",
    "                \"id\": essay_id,\n",
    "                \"text\": essay_text,\n",
    "                \"entities\": essay_entities\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame.from_dict(tagged_essays_list)\n",
    "\n",
    "\n",
    "def generate_file(generation_func, file_path, generate_file=False, *args):\n",
    "    try:\n",
    "        if generate_file:\n",
    "            generation_func(*args).to_csv(file_path, index=False)\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as err:\n",
    "        print(f\"{err}, {type(err)}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Unexpected {err}, {type(err)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving file paths for different folders and files in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = Config.get_all_file_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_metadata = pd.read_csv(file_paths[\"train\"])\n",
    "essays_metadata[['discourse_id', 'discourse_start', 'discourse_end']] = essays_metadata[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n",
    "\n",
    "sample_submission = pd.read_csv(file_paths[\"sample_submission\"])\n",
    "\n",
    "#The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\n",
    "train_txt = glob(file_paths[\"train_folder\"] + \"/*.txt\") \n",
    "test_txt = glob(file_paths[\"test_folder\"] + \"/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...\n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...\n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...\n",
       "3  001552828BD0  Would you be able to give your car up? Having ...\n",
       "4  0016926B079C  I think that students would benefit from learn..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_essays_file = False\n",
    "essays_file_path = file_paths[\"intermediate\"]+\"/train_text.csv\"\n",
    "\n",
    "essays = generate_file(read_essays, essays_file_path, create_essays_file, train_txt)\n",
    "\n",
    "print(essays.shape)\n",
    "essays.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "      <td>[O, O, B-Position, I-Position, I-Position, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...   \n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...   \n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...   \n",
       "3  001552828BD0  Would you be able to give your car up? Having ...   \n",
       "4  0016926B079C  I think that students would benefit from learn...   \n",
       "\n",
       "                                            entities  \n",
       "0  [B-Position, I-Position, I-Position, I-Positio...  \n",
       "1  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "2  [O, O, B-Position, I-Position, I-Position, I-P...  \n",
       "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "4  [B-Position, I-Position, I-Position, I-Positio...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_essay_entities_file = False\n",
    "essay_entities_file_path = file_paths[\"model_input\"]+\"/essays_NER.csv\"\n",
    "\n",
    "essays_entities = generate_file(tag_essays, essay_entities_file_path, create_essay_entities_file, essays, essays_metadata)\n",
    "essays_entities.entities = essays_entities.entities.apply(lambda x: literal_eval(x) )\n",
    "\n",
    "print(essays_entities.shape)\n",
    "essays_entities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-Lead': 1,\n",
       " 'I-Lead': 2,\n",
       " 'B-Position': 3,\n",
       " 'I-Position': 4,\n",
       " 'B-Evidence': 5,\n",
       " 'I-Evidence': 6,\n",
       " 'B-Claim': 7,\n",
       " 'I-Claim': 8,\n",
       " 'B-Concluding Statement': 9,\n",
       " 'I-Concluding Statement': 10,\n",
       " 'B-Counterclaim': 11,\n",
       " 'I-Counterclaim': 12,\n",
       " 'B-Rebuttal': 13,\n",
       " 'I-Rebuttal': 14}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def generate_labels_file(essays_metadata, file_path=file_paths[\"model_input\"]):\n",
    "    label_list = []\n",
    "    label_list.append('O')\n",
    "\n",
    "    for discourse_type in essays_metadata.discourse_type.unique():\n",
    "        label_list.append(f'B-{discourse_type}')\n",
    "        label_list.append(f'I-{discourse_type}')\n",
    "\n",
    "    labels_to_ids = {v:k for k,v in enumerate(label_list)}\n",
    "    ids_to_labels = {k:v for k,v in enumerate(label_list)}\n",
    "\n",
    "    json.dump(labels_to_ids, open(file_path+\"/labels_to_ids.txt\",'w'))\n",
    "    json.dump(ids_to_labels, open(file_path+\"/ids_to_labels.txt\",'w'))\n",
    "\n",
    "\n",
    "generate_labels_file = True\n",
    "if generate_labels_file:\n",
    "    generate_labels_file(essays_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, max_len):\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.sentences[index]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        word_ids = encoding.word_ids()\n",
    "        split_word_ids = np.full(len(word_ids),-1)\n",
    "        offset_to_wordidx = split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "        \n",
    "        mask = encoding['attention_mask']\n",
    "        label = self.labels[index]\n",
    "        label.extend([4]*200)\n",
    "        label=label[:200]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(word_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'tags': torch.tensor(label, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def split_mapping(unsplit):\n",
    "        # Return an array that maps character index to index of word in list of split() words\n",
    "        # Code copied from https://www.kaggle.com/chasembowers/pytorch-bigbird-whitespace-cv-0-6284/notebook\n",
    "        splt = unsplit.split()\n",
    "        no_token_value = -1\n",
    "        offset_to_wordidx = np.full(len(unsplit), no_token_value)\n",
    "        txt_ptr = 0\n",
    "        for split_index, full_word in enumerate(splt):\n",
    "            while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n",
    "                txt_ptr += 1\n",
    "            offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n",
    "            txt_ptr += len(full_word)\n",
    "        return offset_to_wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "sentences = essays_entities.text\n",
    "labels = essays_entities.entities\n",
    "max_len = 1024\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
    "training_set = dataset(\n",
    "    tokenizer=tokenizer, \n",
    "    sentences=sentences, \n",
    "    labels=labels, \n",
    "    max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "      <td>[O, O, B-Position, I-Position, I-Position, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...   \n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...   \n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...   \n",
       "3  001552828BD0  Would you be able to give your car up? Having ...   \n",
       "4  0016926B079C  I think that students would benefit from learn...   \n",
       "\n",
       "                                            entities  \n",
       "0  [B-Position, I-Position, I-Position, I-Positio...  \n",
       "1  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "2  [O, O, B-Position, I-Position, I-Position, I-P...  \n",
       "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "4  [B-Position, I-Position, I-Position, I-Positio...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TEXT AND WORD LABELS \n",
    "index = 0 \n",
    "text = essays_entities.text[index]        \n",
    "get_wids = False\n",
    "word_labels = essays_entities.entities[index] if not get_wids else None\n",
    "\n",
    "# TOKENIZE TEXT\n",
    "encoding = tokenizer(text,\n",
    "                        return_offsets_mapping=True, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        max_length=max_len)\n",
    "\n",
    "word_ids = encoding.word_ids()  \n",
    "split_word_ids = np.full(len(word_ids),-1)\n",
    "offset_to_wordidx = split_mapping(text)\n",
    "offsets = encoding['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_idx = 1\n",
    "print(offsets[token_idx][0])\n",
    "print(offsets[token_idx][1])\n",
    "offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([   0, 6323,   82,  ...,    1,    1,    1]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'offset_mapping': tensor([[ 0,  0],\n",
      "        [ 0,  4],\n",
      "        [ 5, 11],\n",
      "        ...,\n",
      "        [ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  0]]), 'labels': tensor([-100,    3,    4,  ..., -100, -100, -100])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_label_ids():\n",
    "    # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "    label_ids = []\n",
    "    # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "    for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "        if word_idx is None:\n",
    "            if not get_wids: label_ids.append(-100)\n",
    "        else:\n",
    "            if offsets[token_idx] != (0,0):\n",
    "                #Choose the split word that shares the most characters with the token if any\n",
    "                split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "                \n",
    "                if split_index != -1: \n",
    "                    if not get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n",
    "                    split_word_ids[token_idx] = split_index\n",
    "                else:\n",
    "                    # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                    if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n",
    "                        split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                        if not get_wids: label_ids.append(label_ids[-1])\n",
    "                    else:\n",
    "                        if not get_wids: label_ids.append(-100)\n",
    "            else:\n",
    "                if not get_wids: label_ids.append(-100)\n",
    "    return list(reversed(label_ids))\n",
    "\n",
    "encoding['labels'] = get_label_ids()\n",
    "\n",
    "# CONVERT TO TORCH TENSORS\n",
    "item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "if get_wids: \n",
    "    item['wids'] = torch.as_tensor(split_word_ids)\n",
    "\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['labels'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids # for validation????\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        word_labels = self.data.entities[index] if not self.get_wids else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(text,\n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        word_ids = encoding.word_ids()  \n",
    "        split_word_ids = np.full(len(word_ids),-1)\n",
    "        offset_to_wordidx = split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "        \n",
    "        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "        label_ids = []\n",
    "        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "            \n",
    "            if word_idx is None:\n",
    "                if not self.get_wids: label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx] != (0,0):\n",
    "                    #Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "                    \n",
    "                    if split_index != -1: \n",
    "                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if not self.get_wids: label_ids.append(label_ids[-1])\n",
    "                        else:\n",
    "                            if not self.get_wids: label_ids.append(-100)\n",
    "                else:\n",
    "                    if not self.get_wids: label_ids.append(-100)\n",
    "        \n",
    "        encoding['labels'] = list(reversed(label_ids))\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids: \n",
    "            item['wids'] = torch.as_tensor(split_word_ids)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead\n",
      "Position\n",
      "Evidence\n",
      "Claim\n",
      "Concluding Statement\n",
      "Counterclaim\n",
      "Rebuttal\n"
     ]
    }
   ],
   "source": [
    "for type_ in essays_metadata.discourse_type.unique():\n",
    "    print(type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "azureml_py38_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
