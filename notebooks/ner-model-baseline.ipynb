{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changing working directory to repository path \n",
    "in order to make simpler references to files/folder.\n",
    "\n",
    "Also, adding src folder in the repository to import\n",
    "any code that has been moved to py files for reusability\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "REPOSITORY_PATH = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/mlgpu2/code/Users/santiago.a.diez/evaluating-student-writing-kaggle-challenge'\n",
    "os.chdir(REPOSITORY_PATH)\n",
    "import sys  \n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from eswkg.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import cuda\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "#from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_essay(essay_id, folder_path = Config.get_file_path(\"train_folder\")):\n",
    "    with open(folder_path + f\"/{essay_id}.txt\") as f:\n",
    "        essay = f.read()\n",
    "    return essay\n",
    "\n",
    "\n",
    "def read_essays(train_txt):\n",
    "    train_txt_file_id, train_txt_file_text = [],[]\n",
    "    for train_txt_file in train_txt:\n",
    "        essay_id = os.path.basename(train_txt_file).rsplit(\".\",1)[0]\n",
    "        essay_folder = os.path.dirname(train_txt_file)\n",
    "\n",
    "        train_txt_file_id.append(essay_id)\n",
    "        train_txt_file_text.append(read_essay(essay_id,essay_folder))\n",
    "    return pd.DataFrame({\"id\":train_txt_file_id, \"text\":train_txt_file_text})\n",
    "\n",
    "\n",
    "def get_essay_entities(essay_text, essay_metadata):\n",
    "    essay_entities = [\"O\"]*len(essay_text.split())\n",
    "    for discourse_type, predictionstring in zip(essay_metadata[\"discourse_type\"],essay_metadata[\"predictionstring\"]):\n",
    "        predictionstring_digits = list(map(int, predictionstring.split()))\n",
    "        \n",
    "        essay_entities[predictionstring_digits[0]] = f\"B-{discourse_type}\"\n",
    "        for predictionstring_digits_index in predictionstring_digits[1:]:\n",
    "           essay_entities[predictionstring_digits_index] = f\"I-{discourse_type}\"\n",
    "    \n",
    "    return essay_entities\n",
    "\n",
    "\n",
    "def tag_essays(essays, essays_metadata):\n",
    "    tagged_essays = pd.DataFrame()\n",
    "    tagged_essays_list = []\n",
    "    for _, essay in essays.iterrows():\n",
    "        essay_id = essay[\"id\"]\n",
    "        essay_text = essay[\"text\"]\n",
    "        essay_metadata = essays_metadata.query(\"id == @essay_id\")\n",
    "        essay_entities = get_essay_entities(essay_text, essay_metadata)\n",
    "\n",
    "        tagged_essays_list.append( \n",
    "            {\n",
    "                \"id\": essay_id,\n",
    "                \"text\": essay_text,\n",
    "                \"entities\": essay_entities\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame.from_dict(tagged_essays_list)\n",
    "\n",
    "\n",
    "def generate_file(generation_func, file_path, generate_file=False, *args):\n",
    "    try:\n",
    "        if generate_file:\n",
    "            generation_func(*args).to_csv(file_path, index=False)\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as err:\n",
    "        print(f\"{err}, {type(err)}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Unexpected {err}, {type(err)}\")\n",
    "        raise\n",
    "\n",
    "def generate_labels_file(essays_metadata, file_path=Config.get_file_path(\"model_input\")):\n",
    "    label_list = []\n",
    "    label_list.append('O')\n",
    "\n",
    "    for discourse_type in essays_metadata.discourse_type.unique():\n",
    "        label_list.append(f'B-{discourse_type}')\n",
    "        label_list.append(f'I-{discourse_type}')\n",
    "\n",
    "    labels_to_ids = {v:k for k,v in enumerate(label_list)}\n",
    "    ids_to_labels = {k:v for k,v in enumerate(label_list)}\n",
    "    \n",
    "    with open(file_path+\"/label_list.txt\", \"w\") as output:\n",
    "        output.write(str(label_list))\n",
    "        \n",
    "    json.dump(labels_to_ids, open(file_path+\"/labels_to_ids.json\",'w'))\n",
    "    json.dump(ids_to_labels, open(file_path+\"/ids_to_labels.json\",'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving file paths for different folders and files in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = Config.get_all_file_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_metadata = pd.read_csv(file_paths[\"train\"])\n",
    "essays_metadata[['discourse_id', 'discourse_start', 'discourse_end']] = essays_metadata[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n",
    "\n",
    "sample_submission = pd.read_csv(file_paths[\"sample_submission\"])\n",
    "\n",
    "#The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\n",
    "train_txt = glob(file_paths[\"train_folder\"] + \"/*.txt\") \n",
    "test_txt = glob(file_paths[\"test_folder\"] + \"/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>During a group project, have you ever asked a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>80% of Americans believe seeking multiple opin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>When people ask for advice,they sometimes talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Have you ever asked more than one person for h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0FB0700DAF44  During a group project, have you ever asked a ...\n",
       "1  18409261F5C2  80% of Americans believe seeking multiple opin...\n",
       "2  D46BCB48440A  When people ask for advice,they sometimes talk...\n",
       "3  D72CB1C11673  Making choices in life can be very difficult. ...\n",
       "4  DF920E0A7337  Have you ever asked more than one person for h..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_test_essays_file = False\n",
    "essays_file_path = file_paths[\"intermediate\"]+\"/test_text.csv\"\n",
    "\n",
    "test_essays = generate_file(read_essays, essays_file_path, create_test_essays_file, test_txt)\n",
    "\n",
    "print(test_essays.shape)\n",
    "test_essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...\n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...\n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...\n",
       "3  001552828BD0  Would you be able to give your car up? Having ...\n",
       "4  0016926B079C  I think that students would benefit from learn..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_essays_file = False\n",
    "essays_file_path = file_paths[\"intermediate\"]+\"/train_text.csv\"\n",
    "\n",
    "essays = generate_file(read_essays, essays_file_path, create_essays_file, train_txt)\n",
    "\n",
    "print(essays.shape)\n",
    "essays.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "      <td>[O, O, B-Position, I-Position, I-Position, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...   \n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...   \n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...   \n",
       "3  001552828BD0  Would you be able to give your car up? Having ...   \n",
       "4  0016926B079C  I think that students would benefit from learn...   \n",
       "\n",
       "                                            entities  \n",
       "0  [B-Position, I-Position, I-Position, I-Positio...  \n",
       "1  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "2  [O, O, B-Position, I-Position, I-Position, I-P...  \n",
       "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "4  [B-Position, I-Position, I-Position, I-Positio...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_essay_entities_file = False\n",
    "essay_entities_file_path = file_paths[\"model_input\"]+\"/essays_NER.csv\"\n",
    "\n",
    "essays_entities = generate_file(tag_essays, essay_entities_file_path, create_essay_entities_file, essays, essays_metadata)\n",
    "essays_entities.entities = essays_entities.entities.apply(lambda x: literal_eval(x) )\n",
    "\n",
    "print(essays_entities.shape)\n",
    "essays_entities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_labels_file_ = True\n",
    "if generate_labels_file_:\n",
    "    generate_labels_file(essays_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, max_len, get_wids=False, labels_file_path = file_paths[\"model_input\"]):\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids\n",
    "        \n",
    "        with open(labels_file_path+\"/ids_to_labels.json\") as f:\n",
    "            self.ids_to_labels = {int(k):v for k,v in json.load(f).items() }\n",
    "        with open(labels_file_path+\"/labels_to_ids.json\") as f:\n",
    "            self.labels_to_ids = {k:int(v) for k,v in json.load(f).items() }\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.sentences[index]\n",
    "        word_labels = self.labels[index] if not self.get_wids else None\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        encoding['labels'], split_word_ids = self._get_label_ids(text, word_labels, encoding)\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids: \n",
    "            item['wids'] = torch.as_tensor(split_word_ids)\n",
    "        \n",
    "        return item \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def _get_label_ids(self, text, word_labels, encoding):\n",
    "        word_ids = encoding.word_ids()  \n",
    "        split_word_ids = np.full(len(word_ids),-1)\n",
    "        offset_to_wordidx = self._split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "        \n",
    "        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "        label_ids = []\n",
    "        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "            if word_idx is None:\n",
    "                if not self.get_wids: label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx] != (0,0):\n",
    "                    #Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "                    \n",
    "                    if split_index != -1: \n",
    "                        if not self.get_wids: label_ids.append( self.labels_to_ids[word_labels[split_index]] )\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        last_label_id = label_ids[-1]\n",
    "                        if label_ids and last_label_id != -100 and self.ids_to_labels[last_label_id][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if not self.get_wids: label_ids.append(last_label_id)\n",
    "                        else:\n",
    "                            if not self.get_wids: label_ids.append(-100)\n",
    "                else:\n",
    "                    if not self.get_wids: label_ids.append(-100)\n",
    "        \n",
    "        return list(reversed(label_ids)), split_word_ids\n",
    "\n",
    "    def _split_mapping(self, unsplit):\n",
    "        # Return an array that maps character index to index of word in list of split() words\n",
    "        # Code copied from https://www.kaggle.com/chasembowers/pytorch-bigbird-whitespace-cv-0-6284/notebook\n",
    "        splt = unsplit.split()\n",
    "        no_token_value = -1\n",
    "        offset_to_wordidx = np.full(len(unsplit), no_token_value)\n",
    "        txt_ptr = 0\n",
    "        for split_index, full_word in enumerate(splt):\n",
    "            while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n",
    "                txt_ptr += 1\n",
    "            offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n",
    "            txt_ptr += len(full_word)\n",
    "        return offset_to_wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_split(sentences, labels, tokenizer, split_size=0.8):\n",
    "    train_set = dataset(\n",
    "        tokenizer=tokenizer, \n",
    "        sentences=sentences, \n",
    "        labels=labels, \n",
    "        max_len=config[\"max_length\"] )\n",
    "\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    # Before\n",
    "    print('Train data set:', len(train_set))\n",
    "    #print('Test data set:', len(test_set))\n",
    "\n",
    "    train_set_size = int(len(train_set) * split_size)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size])\n",
    "    \n",
    "    # After\n",
    "    print('='*30)\n",
    "    print(\"{:.0%} train split\".format(split_size))\n",
    "    print('Train data set:', len(train_set))\n",
    "    #print('Test data set:', len(test_set))\n",
    "    print('Valid data set:', len(valid_set))\n",
    "\n",
    "    return train_set, valid_set\n",
    "\n",
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "def train(model, optimizer, train_loader, device_config=\"cpu\", grad_norm=10):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    #tr_preds, tr_labels = [], []\n",
    "    \n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(device_config, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device_config, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device_config, dtype = torch.long)\n",
    "\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n",
    "                               return_dict=False)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 200==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n",
    "            wandb.log({\"step\": idx})\n",
    "            wandb.log({\"loss_step\": loss_step})\n",
    "\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        #tr_labels.extend(labels)\n",
    "        #tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=grad_norm\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msdsantiagodiez\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/sdsantiagodiez/evaluating-student-writing-kaggle-challenge/runs/2lnr8knb\" target=\"_blank\">blooming-music-2</a></strong> to <a href=\"https://wandb.ai/sdsantiagodiez/evaluating-student-writing-kaggle-challenge\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_enabled = True\n",
    "\n",
    "wand_project = \"evaluating-student-writing-kaggle-challenge\"\n",
    "wand_entity = \"sdsantiagodiez\"\n",
    "if wandb_enabled:\n",
    "    wandb.init(project=wand_project, entity=wand_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "model_name_alphanumeric = re.sub(\"[^0-9a-zA-Z]+\", \"_\", model_name)\n",
    "\n",
    "train_set_size_proportion = 0.8\n",
    "random_seed = 42\n",
    "\n",
    "config = {'model_name': model_name,   \n",
    "         'max_length': 512,\n",
    "         'train_batch_size':4,\n",
    "         'valid_batch_size':4,\n",
    "         'epochs':5,\n",
    "         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "         'max_grad_norm':10,\n",
    "         'device': 'cuda' if cuda.is_available() else 'cpu'}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "\n",
    "#just renaming and reassigning for readability purposes\n",
    "train_essays_sentences = essays_entities.text\n",
    "train_essays_labels = essays_entities.entities\n",
    "test_essays_sentenes = test_essays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set: 15594\n",
      "==============================\n",
      "80% train split\n",
      "Train data set: 12475\n",
      "Valid data set: 3119\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set = get_train_valid_split(\n",
    "        sentences = train_essays_sentences, \n",
    "        labels = train_essays_labels, \n",
    "        tokenizer = tokenizer,\n",
    "        split_size = train_set_size_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN DATASET AND VALID DATASET\n",
    "train_params = {'batch_size': config['train_batch_size'],\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': config['valid_batch_size'],\n",
    "                'shuffle': False,\n",
    "                'num_workers': 2,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "valid_loader = DataLoader(valid_set, **test_params)\n",
    "\n",
    "# TEST DATASET\n",
    "test_set = dataset(test_essays_sentenes, tokenizer, config['max_length'], True)\n",
    "test_loader = DataLoader(test_essays, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# CREATE MODEL\n",
    "config_model = AutoConfig.from_pretrained(config[\"model_name\"]) \n",
    "\n",
    "config_model.num_labels = 15\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(config[\"model_name\"], config=config_model)\n",
    "model.to(config['device'])\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 1\n",
    "load_saved_model = False\n",
    "train_model = False\n",
    "\n",
    "model_file_name = f'{model_name_alphanumeric}_v{model_version}.pt'\n",
    "model_file_path = f'{file_paths[\"models\"] }/{model_name_alphanumeric}_v{model_version}.pt'\n",
    "\n",
    "if load_saved_model:\n",
    "    model.load_state_dict(torch.load(model_file_path))\n",
    "    print(f\"Model {model_file_name} loaded.\")\n",
    "\n",
    "if train_model:\n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"### Training epoch: {epoch + 1}\")\n",
    "        for g in optimizer.param_groups: \n",
    "            g['lr'] = config['learning_rates'][epoch]\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'### LR = {lr}\\n')\n",
    "\n",
    "        train(model, optimizer, train_loader, device_config=config['device'], grad_norm=config['max_grad_norm'])    \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    torch.save(model.state_dict(), model_file_path)\n",
    "    print(f\"Training complete and model {model_file_name} saved.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "azureml_py38_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
